{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9144ce3f-53a3-40a5-8bbe-6b55d5e458d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 81 articles → asiatime_banker_news.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "아시아타임즈 ‘[은행가소식]’ 기사 크롤러 (통합 버전)\n",
    "- 목록: https://www.asiatime.co.kr/list/31?page=N\n",
    "- 제목에 [은행가소식] 이 포함된 기사만 수집\n",
    "- 날짜 파싱: meta → <time> → ‘입력 :’·‘등록 :’ 정규식 3단계\n",
    "- 본문 파싱: 광고·SNS·저작권 단락 제외하고 20자 이상만 모아 합침\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# ────────────────────── 기본 설정 ──────────────────────\n",
    "BASE_URL  = \"https://www.asiatime.co.kr\"\n",
    "LIST_PATH = \"/list/31\"                          # ‘은행’ 카테고리 ID\n",
    "HEADERS   = {\"User-Agent\": \"Mozilla/5.0 (compatible; BankerBot/1.0)\"}\n",
    "BANKER_RX = re.compile(r\"\\[은행가소식]\")\n",
    "\n",
    "# ────────────────────── 공통 유틸 ──────────────────────\n",
    "def fetch(url: str, timeout: int = 10) -> BeautifulSoup:\n",
    "    \"\"\"URL → BeautifulSoup 객체 반환\"\"\"\n",
    "    res = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "    res.raise_for_status()\n",
    "    return BeautifulSoup(res.text, \"lxml\")\n",
    "\n",
    "# ────────────────────── 날짜 파서 ──────────────────────\n",
    "def parse_date(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"meta → <time> → ‘입력/등록 :’ 정규식 순으로 날짜·시간 추출\"\"\"\n",
    "    META_KEYS = [\n",
    "        (\"property\", \"article:published_time\"),\n",
    "        (\"property\", \"og:article:published_time\"),\n",
    "        (\"name\",     \"pubdate\"),\n",
    "        (\"name\",     \"publish-date\"),\n",
    "        (\"name\",     \"date\"),\n",
    "    ]\n",
    "    # ① meta 태그\n",
    "    for attr, key in META_KEYS:\n",
    "        m = soup.find(\"meta\", {attr: key})\n",
    "        if m and m.get(\"content\"):\n",
    "            ts = m[\"content\"].strip().split()\n",
    "            if len(ts) >= 2:\n",
    "                return ts[0] + \" \" + ts[1][:5]\n",
    "\n",
    "    # ② <time> 태그\n",
    "    t = soup.find(\"time\")\n",
    "    if t:\n",
    "        if t.get(\"datetime\"):\n",
    "            return t[\"datetime\"][:16]\n",
    "        txt = t.get_text(\" \", strip=True)\n",
    "        if re.search(r\"\\d{4}[-./]\\d{2}[-./]\\d{2}\", txt):\n",
    "            return txt.strip()\n",
    "\n",
    "    # ③ 본문 텍스트 패턴\n",
    "    m = re.search(\n",
    "        r\"(입력|등록|게재)\\s*[:·]?\\s*(\\d{4}[-./]\\d{2}[-./]\\d{2}\\s*\\d{2}:\\d{2})\",\n",
    "        soup.get_text(\" \", strip=True)\n",
    "    )\n",
    "    return m.group(2) if m else \"Unknown\"\n",
    "\n",
    "# ────────────────────── 본문 파서 ──────────────────────\n",
    "AD_CLASSES  = {\"pc_article_ad_01\", \"m_article_ad_02\"}\n",
    "AD_PATTERNS = re.compile(r\"advertisement|sns|무단전재|저작권\", re.I)\n",
    "\n",
    "def parse_body(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    - .article-body 또는 #article-view-content-div → 없으면 soup 전체\n",
    "    - <p>, <figcaption> 중 길이 20자↑ & 광고 키워드 없는 것만 추출\n",
    "    \"\"\"\n",
    "    container = (\n",
    "        soup.select_one(\".article-body\")\n",
    "        or soup.select_one(\"#article-view-content-div\")\n",
    "        or soup\n",
    "    )\n",
    "\n",
    "    paras = []\n",
    "    for tag in container.find_all([\"p\", \"figcaption\"], recursive=True):\n",
    "        # 광고 영역 자체 또는 그 내부는 제외\n",
    "        if tag.find_parent(class_=lambda c: c in AD_CLASSES if c else False):\n",
    "            continue\n",
    "        if any(c in AD_CLASSES for c in (tag.get(\"class\") or [])):\n",
    "            continue\n",
    "\n",
    "        text = tag.get_text(\" \", strip=True).replace(\"\\xa0\", \" \").strip()\n",
    "        if len(text) < 20:\n",
    "            continue\n",
    "        if AD_PATTERNS.search(text):\n",
    "            continue\n",
    "\n",
    "        paras.append(text)\n",
    "\n",
    "    return \"\\n\".join(paras)\n",
    "\n",
    "# ────────────────────── 기사 파서 ──────────────────────\n",
    "def parse_article(url: str) -> dict:\n",
    "    soup = fetch(url)\n",
    "    title_tag = soup.find(\"h1\") or soup.find(\"h2\")\n",
    "    title = title_tag.get_text(\" \", strip=True) if title_tag else \"No title\"\n",
    "    author_tag = soup.select_one(\".writer, .author\")\n",
    "    author = author_tag.get_text(strip=True) if author_tag else \"Unknown\"\n",
    "\n",
    "    return {\n",
    "        \"url\":     url,\n",
    "        \"title\":   title,\n",
    "        \"date\":    parse_date(soup),\n",
    "        \"author\":  author,\n",
    "        \"content\": parse_body(soup),\n",
    "    }\n",
    "\n",
    "# ────────────────────── 목록 파서 ──────────────────────\n",
    "def banker_links(list_soup: BeautifulSoup) -> list[str]:\n",
    "    links = []\n",
    "    for a in list_soup.select(\"a[href^='/article/']\"):\n",
    "        if BANKER_RX.search(a.get_text(strip=True)):\n",
    "            links.append(urljoin(BASE_URL, a[\"href\"].split(\"?\")[0]))\n",
    "    return links\n",
    "\n",
    "# ────────────────────── 크롤러 진입점 ──────────────────────\n",
    "def crawl_banker_news(max_pages: int = 10, delay: float = 1.0) -> pd.DataFrame:\n",
    "    rows, seen = [], set()\n",
    "    for page in range(1, max_pages + 1):\n",
    "        list_url = f\"{BASE_URL}{LIST_PATH}?page={page}\"\n",
    "        list_soup = fetch(list_url)\n",
    "        links = banker_links(list_soup)\n",
    "\n",
    "        # [은행가소식]이 더 이상 없으면 조기 종료\n",
    "        if not links:\n",
    "            break\n",
    "\n",
    "        for link in links:\n",
    "            if link in seen:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(parse_article(link))\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] {link} skipped: {e}\")\n",
    "            seen.add(link)\n",
    "            time.sleep(delay)                      # 서버 부하 방지\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ────────────────────── 실행 예시 ──────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_banker_news(max_pages=10, delay=1.0)\n",
    "    df.to_csv(\"asiatime_banker_news.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved {len(df)} articles → asiatime_banker_news.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdde1843-ec09-44d7-8308-cf1aae138d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "463개 단락 저장 완료 → asiatime_banker_sections.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1) 크롤러가 만든 파일 로드\n",
    "df = pd.read_csv(\"asiatime_banker_news.csv\")\n",
    "\n",
    "# 2) '\\n□' 로 분리 → 리스트를 새 열(section)에 담고 explode로 길게 변환\n",
    "df[\"section\"] = df[\"content\"].str.split(r\"\\n□\\s*\")       # \\s* = 기호 뒤 공백 무시\n",
    "long_df = df.explode(\"section\", ignore_index=True)\n",
    "\n",
    "# 3) 공백·빈 문자열 제거\n",
    "long_df[\"section\"] = long_df[\"section\"].str.strip()\n",
    "long_df = long_df[long_df[\"section\"] != \"\"]\n",
    "\n",
    "# 4) 필요한 컬럼만 남기고 한글 이름으로 바꿔 주기\n",
    "long_df = (\n",
    "    long_df.rename(columns={\"date\": \"작성일\", \"url\": \"링크\", \"section\": \"내용\"})\n",
    "           .loc[:, [\"작성일\", \"링크\", \"내용\"]]\n",
    ")\n",
    "\n",
    "# 5) 저장\n",
    "long_df.to_csv(\"asiatime_banker_sections.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"{len(long_df):,}개 단락 저장 완료 → asiatime_banker_sections.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79ecad1c-251c-480c-8ff8-aeba913b98be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>작성일</th>\n",
       "      <th>링크</th>\n",
       "      <th>내용</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-05-22 18:37</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250522500400</td>\n",
       "      <td>유승열 기자 입력 2025-05-22 18:37 수정 2025-05-22 18:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-05-22 18:37</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250522500400</td>\n",
       "      <td>은행권 공동 본인확인서비스 MOU 체결\\n국민·농협·신한·우리·하나·기업은행은 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-05-22 18:37</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250522500400</td>\n",
       "      <td>은행연합회 유럽 금융기관과 협력강화\\n은행연합회는 5월 20일부터 21일까지 독일과...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-05-22 18:37</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250522500400</td>\n",
       "      <td>KB금융공익재단, 해군 재경근무지원대대 장병 대상 경제·금융교육 실시\\nKB금융공익...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-05-22 18:37</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250522500400</td>\n",
       "      <td>신한은행 노란우산 가입 소상공인 금융지원 실시\\n신한은행은 22일 중소기업중앙회, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>458</th>\n",
       "      <td>2025-03-20 17:53</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250320500402</td>\n",
       "      <td>정종진 기자 입력 2025-03-20 17:53 수정 2025-03-20 17:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>2025-03-20 17:53</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250320500402</td>\n",
       "      <td>전북은행 '봄날 특판 예금' 출시\\n전북은행이 봄맞이 이벤트 우대금리를 제공하는 '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>2025-03-20 17:53</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250320500402</td>\n",
       "      <td>토스뱅크 '수출 소상공인 지원' 무보와 맞손\\n토스뱅크가 한국무역보험공사와 협력해 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>2025-03-20 17:53</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250320500402</td>\n",
       "      <td>농협은행 '우리 쌀 꾸러미' 전달\\n농협은행 기업금융부문이 19일 경기도 화성시 소...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>2025-03-20 17:53</td>\n",
       "      <td>https://www.asiatime.co.kr/article/20250320500402</td>\n",
       "      <td>부산은행 '연금계좌 바꿔봄' 이벤트\\n부산은행은 오는 6월 30일까지 4대연금(국민...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>463 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  작성일                                                 링크  \\\n",
       "0    2025-05-22 18:37  https://www.asiatime.co.kr/article/20250522500400   \n",
       "1    2025-05-22 18:37  https://www.asiatime.co.kr/article/20250522500400   \n",
       "2    2025-05-22 18:37  https://www.asiatime.co.kr/article/20250522500400   \n",
       "3    2025-05-22 18:37  https://www.asiatime.co.kr/article/20250522500400   \n",
       "4    2025-05-22 18:37  https://www.asiatime.co.kr/article/20250522500400   \n",
       "..                ...                                                ...   \n",
       "458  2025-03-20 17:53  https://www.asiatime.co.kr/article/20250320500402   \n",
       "459  2025-03-20 17:53  https://www.asiatime.co.kr/article/20250320500402   \n",
       "460  2025-03-20 17:53  https://www.asiatime.co.kr/article/20250320500402   \n",
       "461  2025-03-20 17:53  https://www.asiatime.co.kr/article/20250320500402   \n",
       "462  2025-03-20 17:53  https://www.asiatime.co.kr/article/20250320500402   \n",
       "\n",
       "                                                    내용  \n",
       "0       유승열 기자 입력 2025-05-22 18:37 수정 2025-05-22 18:37  \n",
       "1    은행권 공동 본인확인서비스 MOU 체결\\n국민·농협·신한·우리·하나·기업은행은 21...  \n",
       "2    은행연합회 유럽 금융기관과 협력강화\\n은행연합회는 5월 20일부터 21일까지 독일과...  \n",
       "3    KB금융공익재단, 해군 재경근무지원대대 장병 대상 경제·금융교육 실시\\nKB금융공익...  \n",
       "4    신한은행 노란우산 가입 소상공인 금융지원 실시\\n신한은행은 22일 중소기업중앙회, ...  \n",
       "..                                                 ...  \n",
       "458     정종진 기자 입력 2025-03-20 17:53 수정 2025-03-20 17:53  \n",
       "459  전북은행 '봄날 특판 예금' 출시\\n전북은행이 봄맞이 이벤트 우대금리를 제공하는 '...  \n",
       "460  토스뱅크 '수출 소상공인 지원' 무보와 맞손\\n토스뱅크가 한국무역보험공사와 협력해 ...  \n",
       "461  농협은행 '우리 쌀 꾸러미' 전달\\n농협은행 기업금융부문이 19일 경기도 화성시 소...  \n",
       "462  부산은행 '연금계좌 바꿔봄' 이벤트\\n부산은행은 오는 6월 30일까지 4대연금(국민...  \n",
       "\n",
       "[463 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8042ecf4-f248-4f15-b588-b8cf4d77fc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "잡문 제거 완료 → asiatime_banker_sections_clean.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, re\n",
    "\n",
    "JUNK = re.compile(r\"기자\\s*입력|입력\\s*[:·]|수정\\s*[:·]|Copyright|All rights reserved\"\n",
    "                  r\"|건전한\\s*토론문화|고충처리인|뉴스제휴위원회\"\n",
    "                  r\"|\\bTEL\\b|\\bemail\\b\", re.I)\n",
    "\n",
    "df = pd.read_csv(\"asiatime_banker_sections.csv\")\n",
    "df[\"내용\"] = df[\"내용\"].str.replace(JUNK, \"\", regex=True).str.strip()\n",
    "df = df[df[\"내용\"].str.len() > 0]        # 완전히 빈 행 삭제\n",
    "df.to_csv(\"asiatime_banker_sections_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(\"잡문 제거 완료 → asiatime_banker_sections_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ce37e97-c8c6-4299-9ee2-72b085e7387e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81건 저장 → asiatime_banker_news_clean.csv\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "아시아타임즈 ‘[은행가소식]’ 크롤러 (footer 제거‧news-only 버전)\n",
    "\"\"\"\n",
    "\n",
    "import re, time, requests, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE_URL  = \"https://www.asiatime.co.kr\"\n",
    "LIST_PATH = \"/list/31\"                       # ‘은행’ 카테고리\n",
    "HEADERS   = {\"User-Agent\": \"Mozilla/5.0 (compatible; BankerBot/2.0)\"}\n",
    "BANKER_RX = re.compile(r\"\\[은행가소식]\")\n",
    "\n",
    "# ─────────────────────── fetch ────────────────────────\n",
    "def fetch(url: str, timeout: int = 10) -> BeautifulSoup:\n",
    "    r = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "    r.raise_for_status()\n",
    "    return BeautifulSoup(r.text, \"lxml\")\n",
    "\n",
    "# ─────────────────────── 날짜 ─────────────────────────\n",
    "def parse_date(soup: BeautifulSoup) -> str:\n",
    "    for meta_key in [\n",
    "        (\"property\", \"article:published_time\"),\n",
    "        (\"property\", \"og:article:published_time\"),\n",
    "        (\"name\",     \"pubdate\"), (\"name\", \"publish-date\"), (\"name\", \"date\")\n",
    "    ]:\n",
    "        m = soup.find(\"meta\", {meta_key[0]: meta_key[1]})\n",
    "        if m and m.get(\"content\"):\n",
    "            ts = m[\"content\"].split()\n",
    "            if len(ts) >= 2:\n",
    "                return ts[0] + \" \" + ts[1][:5]\n",
    "\n",
    "    t = soup.find(\"time\")\n",
    "    if t:\n",
    "        return (t.get(\"datetime\") or t.get_text()).strip()[:16]\n",
    "\n",
    "    m = re.search(r\"(입력|등록)\\s*[:·]?\\s*(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2})\",\n",
    "                  soup.get_text(\" \", strip=True))\n",
    "    return m.group(2) if m else \"Unknown\"\n",
    "\n",
    "# ─────────────────────── 본문 ─────────────────────────\n",
    "# 1) 기사 본문이 시작되기 전에 흔히 보이는 패턴(기자·입력·수정)\n",
    "HEAD_JUNK_RX = re.compile(\n",
    "    r\"(기자\\s*입력|입력\\s*[:·]|수정\\s*[:·]|데스크\\s*:)\", re.I\n",
    ")\n",
    "\n",
    "# 2) 본문이 끝나면 나타나는 푸터(댓글 안내·저작권·고충처리인 …)\n",
    "TAIL_JUNK_RX = re.compile(\n",
    "    r\"Copyright|All\\s+rights\\s+reserved|건전한\\s*토론문화|고충처리인\"\n",
    "    r\"|뉴스제휴위원회|\\bTEL\\b|\\bemail\\b\", re.I\n",
    ")\n",
    "\n",
    "def parse_body(soup: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    ① 본문 container(.article-body / #article-view-content-div / fallback soup)\n",
    "    ② <p>,<figcaption> 순서대로 훑으면서\n",
    "       · HEAD_JUNK_RX가 처음 등장하기 전 문단 skip\n",
    "       · TAIL_JUNK_RX 만나면 break\n",
    "       · 20자 이상 & 광고·SNS 문구 없는 것만 저장\n",
    "    \"\"\"\n",
    "    container = (\n",
    "        soup.select_one(\".article-body\")\n",
    "        or soup.select_one(\"#article-view-content-div\")\n",
    "        or soup\n",
    "    )\n",
    "\n",
    "    AD_CLASSES  = {\"pc_article_ad_01\", \"m_article_ad_02\"}\n",
    "    AD_RX       = re.compile(r\"advertisement|sns|무단전재|저작권\", re.I)\n",
    "\n",
    "    paras, started = [], False\n",
    "\n",
    "    for tag in container.find_all([\"p\", \"figcaption\"], recursive=True):\n",
    "        # 광고 블록 무시\n",
    "        if tag.find_parent(class_=lambda c: c in AD_CLASSES if c else False):\n",
    "            continue\n",
    "        if any(c in AD_CLASSES for c in (tag.get(\"class\") or [])):\n",
    "            continue\n",
    "\n",
    "        text = tag.get_text(\" \", strip=True).replace(\"\\xa0\", \" \").strip()\n",
    "\n",
    "        if not text or len(text) < 20:\n",
    "            continue\n",
    "\n",
    "        # 머리말(기자·입력 정보) 스킵\n",
    "        if not started:\n",
    "            if HEAD_JUNK_RX.search(text):\n",
    "                continue\n",
    "            started = True                       # 여기부터 본문\n",
    "        # 본문 끝 지점이면 종료\n",
    "        if TAIL_JUNK_RX.search(text):\n",
    "            break\n",
    "        # 광고·SNS 안내 같은 노이즈 제거\n",
    "        if AD_RX.search(text):\n",
    "            continue\n",
    "\n",
    "        paras.append(text)\n",
    "\n",
    "    return \"\\n\".join(paras)\n",
    "\n",
    "# ─────────────────────── 기사 파서 ────────────────────\n",
    "def parse_article(url: str) -> dict:\n",
    "    soup = fetch(url)\n",
    "    title_tag = soup.find(\"h1\") or soup.find(\"h2\")\n",
    "    return {\n",
    "        \"url\":   url,\n",
    "        \"date\":  parse_date(soup),\n",
    "        \"content\": parse_body(soup),\n",
    "    }\n",
    "\n",
    "# ─────────────────────── 목록 파서 ────────────────────\n",
    "def banker_links(list_soup: BeautifulSoup) -> list[str]:\n",
    "    links = []\n",
    "    for a in list_soup.select(\"a[href^='/article/']\"):\n",
    "        if BANKER_RX.search(a.get_text(strip=True)):\n",
    "            links.append(urljoin(BASE_URL, a[\"href\"].split(\"?\")[0]))\n",
    "    return links\n",
    "\n",
    "# ─────────────────────── 크롤링 ───────────────────────\n",
    "def crawl_banker(max_pages: int = 10, delay: float = 1.0) -> pd.DataFrame:\n",
    "    rows, seen = [], set()\n",
    "    for page in range(1, max_pages + 1):\n",
    "        soup = fetch(f\"{BASE_URL}{LIST_PATH}?page={page}\")\n",
    "        links = banker_links(soup)\n",
    "        if not links:\n",
    "            break\n",
    "\n",
    "        for link in links:\n",
    "            if link in seen:\n",
    "                continue\n",
    "            try:\n",
    "                art = parse_article(link)\n",
    "                if art[\"content\"]:               # 빈 기사 거르기\n",
    "                    rows.append(art)\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] {link} skipped: {e}\")\n",
    "            seen.add(link)\n",
    "            time.sleep(delay)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ─────────────────────── 실행 예시 ────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    df = crawl_banker(max_pages=10, delay=1.0)\n",
    "    df.to_csv(\"asiatime_banker_news_clean.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"{len(df):,}건 저장 → asiatime_banker_news_clean.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
